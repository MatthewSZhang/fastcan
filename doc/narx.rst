.. currentmodule:: fastcan.narx

.. _narx:

=======================================================
Reduced polynomial NARX model for system identification
=======================================================

:class:`NARX` is a class to build a reduced polynomial NARX (Nonlinear AutoRegressive eXogenous) model for system identification.
The structure of a polynomial NARX model is shown below:

.. math::
    y(k) = 0.5y(k-1) + 0.3u_0(k)^2 + 2u_0(k-1)u_0(k-3) + 1.5u_0(k-2)u_1(k-3) + 1

where :math:`k` is the time index, :math:`u_0` and :math:`u_1` are input signals, and :math:`y` is the output signal.


The NARX model is composed of three parts:

#. Terms: :math:`y(k-1)`, :math:`u_0(k)^2`, :math:`u_0(k-1)u_0(k-3)`, and :math:`u_0(k-2)u_1(k-3)`
#. Coefficients: 0.5, 0.3, 2, and 1.5
#. Intercept: 1


To build a reduced polynomial NARX model, we normally need the following steps:

#. Build term library
    #. Get time shifted variables: e.g., :math:`u(t-2)` and :math:`y(t-1)`
    #. Get terms by nonlinearising the time shifted variables with polynomial basis, e.g., :math:`u_0(k-1)u_0(k-3)` and :math:`u_0(k-2)u_1(k-3)`
#. Search useful terms by feature selection, i.e., `Reduced Order Modelling (ROM) <https://www.mathworks.com/discovery/reduced-order-modeling.html>`_
#. Learn the coefficients by least-squares

.. rubric:: Examples

* See :ref:`sphx_glr_auto_examples_plot_narx.py` for an example of the basic usage of the NARX model.

Multiple time series and missing values
=======================================

For time series modelling, the continuity of the data is important.
The continuity means all neighbouring samples have the same time interval.
A discontinuous time series can be treated as multiple pieces of continuous time series.
The discontinuity can be caused by

#. Different measurement sessions, e.g., one measurement session is carried out at Monday and
   lasts for one hour at 1 Hz, and another measurement is carried out at Tuesday and lasts for two hours at 1 Hz
#. Missing values

No matter how the discontinuity is caused, :class:`NARX` treats the discontinuous time series as multiple pieces of continuous time series in the same way.

.. note::
    It is easy to notify :class:`NARX` that the time series data are from different measurement sessions by inserting np.nan to break the data. For example,

    >>> import numpy as np
    >>> x0 = np.zeros((3, 2)) # First measurement session has 3 samples with 2 features
    >>> x1 = np.ones((5, 2)) # Second measurement session has 5 samples with 2 features
    >>> u = np.r_[x0, [[np.nan, np.nan]], x1] # Insert np.nan to break the two measurement sessions

    It is important to break the different measurement sessions by np.nan, because otherwise,
    the model will assume the time interval between the two measurement sessions is the same as the time interval within a session.


One-step-ahead and multiple-step-ahead prediction
=================================================

In system identification, two types of predictions can be made by NARX models:

#. One-step-ahead prediction: predict the output at the next time step given the measured input and the measured output at the past time steps, e.g., :math:`\hat{y}(k) = 0.5y(k-1) + 0.3u_0(k-1)^2`, where :math:`\hat{y}(k)` is the predicted output at time step :math:`k`
#. Multiple-step-ahead prediction: predict the output at the next time step given the measured input and the predicted output at the past time steps, e.g., :math:`\hat{y}(k) = 0.5\hat{y}(k-1) + 0.3u_0(k-1)^2`

Obviously, the multiple-step-ahead prediction is more challenging, because the predicted output is used as the input to predict the output at the next step, which may accumulate the error.
The `prediction` method of :class:`NARX` gives the multiple-step-ahead prediction.

It should also be noted the different types of predictions in model training.

#. If assume the NARX model is one-step-ahead prediction structure, we know all input data for training in advance,
   and the model can be trained by the simple ordinary least-squares (OLS) method
#. If assume the NARX model is a multiple-step-ahead prediction structure, the input data, like :math:`\hat{y}(k-1)` is
   unknown in advance. Therefore, the training data must first be generated by the multiple-step-ahead prediction with
   the initial model coefficients, and then the coefficients can be updated recursively

ARX and OE model
----------------

To better understand the two types of training, it is helpful to know two linear time series model structures,
i.e., `ARX (AutoRegressive eXogenous) model <https://www.mathworks.com/help/ident/ref/arx.html>`_ and
`OE (output error) model <https://www.mathworks.com/help/ident/ref/oe.html>`_.

The ARX model structure is

.. math::
    (1-A(z))y(t) = B(z)u(t) + e(t)

where :math:`A(z)` and :math:`B(z)` are polynomials of :math:`z`, e.g.,
:math:`A(z) = 1.5 z^{-1} - 0.7 z^{-2}`, :math:`z` is a complex number from `Z-transformation <https://en.wikipedia.org/wiki/Z-transform>`_
and :math:`z^{-1}` can be view as a time-shift operator, and :math:`e(t)` is the white noise.

The OE model structure is

.. math::
    y(t) = \frac{B(z)}{(1-A(z))} u(t) + e(t)


The ARX model and OE model is very similar. If we rewrite ARX model to :math:`y(t) = \frac{B(z)}{(1-A(z))} u(t) + \frac{1}{(1-A(z))} e(t)`,
we can see that both the ARX model and the OE model have the same transfer function :math:`\frac{B(z)}{(1-A(z))}`.
The two key differences are:

#. noise assumption
#. `best linear unbiased estimator (BLUE) <https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem>`_

For noise assumption, the noise of the ARX model passes through a filter :math:`\frac{1}{(1-A(z))}` while the noise of the OE model not.
The structure of ARX models implies that the measurement noise is a very special colored noise, which is not common in practice.

For difference in BLUE, to get BLUE of models, we need to rewrite models to the form of

.. math::
    e(t) = y(t) - \hat{y}(t)

where :math:`\hat{y}(t)` is a predictor, which describes how to compute the predicted output to make the error white.
It is easy to find that for a ARX model

.. math::
    \hat{y}(t) = A(z)y(t) + B(z)u(t)

while for an OE model

.. math::
    \hat{y}(t) = \frac{B(z)}{(1-A(z))} u(t) = A(z)\hat{y}(t) + B(z)u(t)

The difference between the two predictors is the one for ARX models uses both measured input and output to make prediction,
while the one for OE models only uses measured input to make prediction.
In other words, the predictor of the BLUE for ARX computes one-step-ahead prediction,
while the predictor of the BLUE for OE computes multiple-step-ahead prediction.

In summary, using multiple-step-ahead prediction in model training is more aligned with the real noise condition, but harder to train.
However, using one-step-ahead prediction in model training is easier to train, but the noise assumption is too strong to be true.
The `fit` method of :class:`NARX` uses the one-step-ahead prediction for training by default,
but the multiple-step-ahead prediction can be used by setting the parameter `coef_init`.

.. rubric:: Examples

* See :ref:`sphx_glr_auto_examples_plot_narx_msa.py` for an illustration of the multiple-step-ahead NARX model.
